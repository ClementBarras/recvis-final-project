{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from dataset import SupervisedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from utils import get_free_gpu\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from models import O3N\n",
    "from i3d import I3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'random_flow'\n",
    "model_path = 'random_flow/model_6.pth'\n",
    "FRAMES_PER_SEQUENCE = 10\n",
    "\n",
    "datasets_path = \"../datasets/\"\n",
    "video_path = os.path.join(datasets_path, \"UCF101_frames\")\n",
    "features_path = os.path.join(datasets_path, \"precomputed_features\",  model_name)\n",
    "\n",
    "try:\n",
    "    os.mkdir(features_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "GPU usage:\n",
      "  memory.used memory.free\n",
      "0     203 MiB   12003 MiB\n",
      "1     256 MiB   11950 MiB\n",
      "Using GPU0 with 12003.0 free MiB\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "\n",
    "i3d = I3D(num_classes=400)\n",
    "o3n = O3N(i3d=i3d, n_questions=6)\n",
    "o3n.load_state_dict(torch.load(model_path))\n",
    "model = o3n.i3d\n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    free_gpu_id = get_free_gpu()\n",
    "    device = \"cuda:{}\".format(free_gpu_id)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(vid_path, idxs):\n",
    "    frames = []\n",
    "    for idx in idxs:\n",
    "        path = os.path.join(vid_path, \"frame_{}.jpg\".format(idx))\n",
    "        frame = Image.open(path)\n",
    "        frame = transforms.Resize((224, 224))(frame)\n",
    "        frames.append(transforms.ToTensor()(frame))\n",
    "    return torch.stack(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_ApplyLipstick_g01_c03_(4192/13320)\n"
     ]
    }
   ],
   "source": [
    "video_list = [f.name for f in os.scandir(video_path) if f.is_dir()]\n",
    "starting_idx = 0\n",
    "\n",
    "for v, video in enumerate(video_list[starting_idx:]):\n",
    "    clear_output()\n",
    "    print(\"{}_({}/{})\".format(video,v+1+starting_idx, len(video_list)))\n",
    "    video_folder = os.path.join(video_path, video)\n",
    "    res_folder = os.path.join(features_path, video)\n",
    "    #print(res_folder)\n",
    "    try:\n",
    "        os.mkdir(res_folder)\n",
    "    except:\n",
    "        pass\n",
    "    frame_list = [f.name for f in os.scandir(video_folder) if f.is_file()]\n",
    "    frame_count = len(frame_list)\n",
    "    n_sequences = frame_count//FRAMES_PER_SEQUENCE\n",
    "    n_files_in_folder = len(list(os.scandir(res_folder)))\n",
    "    if n_files_in_folder>=n_sequences:\n",
    "        print(\"skipping\")\n",
    "    else:\n",
    "        for seq in range(n_sequences):\n",
    "            start_idx = seq*FRAMES_PER_SEQUENCE + 1\n",
    "            end_idx = start_idx + FRAMES_PER_SEQUENCE - 1\n",
    "            idxs = np.arange(start_idx, end_idx+1)\n",
    "            inputs = extract_frames(video_folder, idxs)[None]\n",
    "            #print(inputs.shape)\n",
    "            inputs = inputs.permute((0,2,1,3,4))\n",
    "            model.forward(inputs.to(device))\n",
    "            features = model.features.cpu().detach().numpy().ravel()\n",
    "            file_name = \"frames_{}_to_{}\".format(start_idx, end_idx)\n",
    "            np.savetxt(os.path.join(res_folder, file_name), features, fmt=\"%.5f\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [f.name for f in os.scandir(video_path) if f.is_dir()]\n",
    "\n",
    "for v, video in enumerate(video_list):\n",
    "    #clear_output()\n",
    "    print(\"{}_({}/{})\".format(video,v+1, len(video_list)))\n",
    "    video_folder = os.path.join(video_path, video)\n",
    "    res_folder = os.path.join(features_path, video)\n",
    "    #print(res_folder)\n",
    "    try:\n",
    "        os.mkdir(res_folder)\n",
    "    except:\n",
    "        pass\n",
    "    frame_list = [f.name for f in os.scandir(video_folder) if f.is_file()]\n",
    "    frame_count = len(frame_list)\n",
    "    n_sequences = frame_count//FRAMES_PER_SEQUENCE\n",
    "    input_list = []\n",
    "    for seq in range(n_sequences):\n",
    "        start_idx = seq*FRAMES_PER_SEQUENCE + 1\n",
    "        end_idx = start_idx + FRAMES_PER_SEQUENCE - 1\n",
    "        idxs = np.arange(start_idx, end_idx+1)\n",
    "        inputs = extract_frames(video_folder, idxs)\n",
    "        input_list.append(inputs)\n",
    "    inputs = torch.stack(input_list)   \n",
    "    inputs = inputs.permute((0,2,1,3,4))\n",
    "    model.forward(inputs.to(device))\n",
    "    features = model.features.cpu().detach().numpy()\n",
    "    print(features.shape)\n",
    "    #file_name = \"frames_{}_to_{}\".format(start_idx, end_idx)\n",
    "    #np.savetxt(os.path.join(res_folder, file_name), features, fmt=\"%.5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.936\n",
      "0.058905058905058906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = np.load('precalculated_features.npy')\n",
    "y_train = np.load('labels.npy')\n",
    "\n",
    "X_test = np.load('precalculated_features_val1.npy')\n",
    "y_test = np.load('labels_val1.npy')\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2886\n",
      "video_id\n",
      "0     13\n",
      "1     13\n",
      "2     13\n",
      "3      2\n",
      "4     13\n",
      "5      2\n",
      "6     13\n",
      "7     13\n",
      "8     13\n",
      "9      9\n",
      "10    13\n",
      "11    15\n",
      "12    13\n",
      "13    13\n",
      "14    13\n",
      "15    17\n",
      "16    13\n",
      "17    13\n",
      "18    13\n",
      "19    13\n",
      "20    17\n",
      "21    17\n",
      "22    13\n",
      "23     9\n",
      "24    13\n",
      "25    13\n",
      "26    13\n",
      "27    13\n",
      "28    13\n",
      "29    13\n",
      "      ..\n",
      "70     9\n",
      "71     9\n",
      "72     9\n",
      "73     9\n",
      "74     9\n",
      "75     9\n",
      "76     9\n",
      "77    13\n",
      "78     9\n",
      "79    13\n",
      "80    13\n",
      "81     9\n",
      "82    13\n",
      "83     9\n",
      "84    13\n",
      "85    13\n",
      "86    13\n",
      "87    13\n",
      "88     9\n",
      "89    13\n",
      "90    13\n",
      "91    13\n",
      "92    15\n",
      "93    13\n",
      "94    13\n",
      "95    13\n",
      "96    13\n",
      "97    13\n",
      "98    13\n",
      "99    13\n",
      "Name: pred_label, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_pred = model.predict_proba(X_test)\n",
    "df = pd.DataFrame(y_pred)\n",
    "df.columns = list(range(22))\n",
    "test = list(range(len(df)))\n",
    "test = [i%100 for i in test]\n",
    "df['video_id'] = test\n",
    "print(len(df))\n",
    "df = df.groupby('video_id').sum()\n",
    "df['pred_label'] = df.idxmax(axis=1)\n",
    "print(df['pred_label'])\n",
    "y_pred = df['pred_label'].values\n",
    "y_real = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_ApplyEyeMakeup_g01_c01\n",
      "v_ApplyEyeMakeup_g01_c01\n",
      "frames_91_to_100\n",
      "frames_101_to_110\n",
      "frames_121_to_130\n",
      "frames_71_to_80\n",
      "frames_31_to_40\n",
      "frames_141_to_150\n",
      "frames_51_to_60\n",
      "frames_61_to_70\n",
      "frames_21_to_30\n",
      "frames_151_to_160\n",
      "frames_81_to_90\n",
      "frames_41_to_50\n",
      "frames_131_to_140\n",
      "frames_1_to_10\n",
      "frames_11_to_20\n",
      "frames_111_to_120\n",
      "v_ApplyEyeMakeup_g01_c02\n",
      "v_ApplyEyeMakeup_g01_c03\n",
      "v_ApplyEyeMakeup_g01_c03\n",
      "frames_91_to_100\n",
      "frames_101_to_110\n",
      "frames_121_to_130\n",
      "frames_171_to_180\n",
      "frames_71_to_80\n",
      "frames_201_to_210\n",
      "frames_31_to_40\n",
      "frames_141_to_150\n",
      "frames_51_to_60\n",
      "frames_61_to_70\n",
      "frames_21_to_30\n",
      "frames_151_to_160\n",
      "frames_211_to_220\n",
      "frames_161_to_170\n",
      "frames_81_to_90\n",
      "frames_191_to_200\n",
      "frames_221_to_230\n",
      "frames_41_to_50\n",
      "frames_131_to_140\n",
      "frames_1_to_10\n",
      "frames_241_to_250\n",
      "frames_181_to_190\n",
      "frames_11_to_20\n",
      "frames_231_to_240\n",
      "frames_111_to_120\n",
      "v_ApplyEyeMakeup_g01_c04\n",
      "v_ApplyEyeMakeup_g01_c05\n",
      "v_ApplyEyeMakeup_g01_c05\n",
      "frames_91_to_100\n",
      "frames_101_to_110\n",
      "frames_121_to_130\n",
      "frames_171_to_180\n",
      "frames_71_to_80\n",
      "frames_201_to_210\n",
      "frames_271_to_280\n",
      "frames_281_to_290\n",
      "frames_31_to_40\n",
      "frames_141_to_150\n",
      "frames_51_to_60\n",
      "frames_251_to_260\n",
      "frames_61_to_70\n",
      "frames_21_to_30\n",
      "frames_151_to_160\n",
      "frames_211_to_220\n",
      ".ipynb_checkpoints\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../datasets/adam_constrained_sampling/v_ApplyEyeMakeup_g01_c05/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/workspace/recvis-final-project/src/create_feature_file.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../datasets/adam_constrained_sampling/v_ApplyEyeMakeup_g01_c05/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "run final_model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        input_list = []\n",
    "        for seq in range(n_sequences):\n",
    "            start_idx = seq*FRAMES_PER_SEQUENCE + 1\n",
    "            end_idx = start_idx + FRAMES_PER_SEQUENCE - 1\n",
    "            idxs = np.arange(start_idx, end_idx+1)\n",
    "            inputs = extract_frames(video_folder, idxs)\n",
    "            input_list.append(inputs)\n",
    "    inputs = torch.stack(input_list)   \n",
    "    inputs = inputs.permute((0,2,1,3,4))\n",
    "    batch_size = len(inputs)\n",
    "    if batch_size > 4:\n",
    "        feature_list = []\n",
    "        for row in inputs:\n",
    "            #print(row.size())\n",
    "            model.forward(row[None,:,:,:,:].to(device))\n",
    "            feature_list.append(model.features)\n",
    "        features = torch.squeeze(torch.stack(feature_list)).cpu().detach().numpy()\n",
    "    else:\n",
    "        #print(inputs.size())\n",
    "        model.forward(inputs.to(device))\n",
    "        features = model.features\n",
    "        features = torch.squeeze(features)\n",
    "        features = features.cpu().detach().numpy()\n",
    "    #print(features.shape)\n",
    "    file_name = \"frames_{}_to_{}\".format(start_idx, end_idx)\n",
    "    #print(os.path.join(res_folder, file_name))\n",
    "    np.savetxt(os.path.join(res_folder, file_name), features, fmt=\"%.5f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
